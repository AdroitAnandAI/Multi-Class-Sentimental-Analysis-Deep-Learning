{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Architectures on Amazon Reviews Dataset (Part II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fine Food Review Dataset\n",
    "\n",
    "\n",
    "**Data Source:** <br>https://www.kaggle.com/snap/amazon-fine-food-reviews<br>\n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of **reviews of fine foods from Amazon.**<br>\n",
    "\n",
    "Number of reviews: **568,454**<br>\n",
    "Number of users: 256,059<br>\n",
    "Number of products: 74,258<br>\n",
    "Timespan: Oct 1999 - Oct 2012<br>\n",
    "Number of Attributes/Columns in data: 10 <br>\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps at a Glance\n",
    "\n",
    "1. Take in **Amazon Review dataset as input**\n",
    "2. **Generate a vocabulary** of all words\n",
    "3. **Make a word-frequency table** having frequency corresponding to each word\n",
    "4. **Generate the index of each word** based on sorted frequency (only top 'n' words are considered)\n",
    "5. Encode the reviews as a set of **indices of top 'n' frequent words**. Remaining words are ignored.\n",
    "6. **Run the LSTM Model on Single Layer & Double-Layer LSTM**, each layer having 100s of LSTMs stacked in parallel. \n",
    "7. **Tune for  best Accuracy by changing the number of neurons** in each layer to compare performance of different architectures.\n",
    "8. Draw the **error plots, of both train and test loss**, for each architurecture to find **whether the model is overfitting** or not.\n",
    "9. Apply regularization such as **Dropout, L1, L2, L1L2 or a combination of these** to reduce overfitting.\n",
    "10. Conclusion based on the accuracy and plots obtained with test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index      Id   ProductId          UserId      ProfileName  \\\n",
      "0  138706  150524  0006641040   ACITT7DI6IDDL  shari zychinski   \n",
      "1  138688  150506  0006641040  A2IW4PEEKO2R0U            Tracy   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
      "0                     0                       0  positive   939340800   \n",
      "1                     1                       1  positive  1194739200   \n",
      "\n",
      "                                      Summary  \\\n",
      "0                   EVERY book is educational   \n",
      "1  Love the book, miss the hard cover version   \n",
      "\n",
      "                                                Text  \\\n",
      "0  this witty little book makes my son laugh at l...   \n",
      "1  I grew up reading these Sendak books, and watc...   \n",
      "\n",
      "                                         CleanedText  \n",
      "0  b'witti littl book make son laugh loud recit c...  \n",
      "1  b'grew read sendak book watch realli rosi movi...  \n"
     ]
    }
   ],
   "source": [
    "#loading libraries for LR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.regularizers import L1L2\n",
    "#from sklearn import cross_validation\n",
    "\n",
    "#loading libraries for scikit learn, nlp, db, plot and matrix.\n",
    "import sqlite3\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# using the SQLite Table to read data.\n",
    "con = sqlite3.connect('./final.sqlite') \n",
    "\n",
    "#filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3\n",
    "final = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "\"\"\", con) \n",
    "\n",
    "print(final.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling & Class Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To randomly sample the data and sort based on time before doing train/ test split.\n",
    "\n",
    "num_points = 100000\n",
    "\n",
    "# used to format headings \n",
    "bold = '\\033[1m'\n",
    "end = '\\033[0m'\n",
    "\n",
    "# you can use random_state for reproducibility\n",
    "# sampled_final = final.sample(n=num_points, random_state=2)\n",
    "\n",
    "#Sorting data according to Time in ascending order\n",
    "sorted_final = final.sort_values('Time', axis=0, \n",
    "                ascending=True, inplace=False, kind='quicksort', na_position='first')\n",
    "\n",
    "final = sorted_final.tail(num_points)\n",
    "\n",
    "# fetching the outcome class \n",
    "y = final['Score'] \n",
    "\n",
    "def class2num(response):\n",
    "    if (response == 'positive'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "y_bin = list(map(class2num, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary and Word-Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary and make sorted Word-Frequency  table\n",
    "\n",
    "import re\n",
    "import collections\n",
    "\n",
    "words = []\n",
    "\n",
    "# Take in all the reviews\n",
    "reviewText = final['CleanedText'].values\n",
    "\n",
    "# Create a list of all words in the db\n",
    "# iterate for each review/sentence\n",
    "for sent in reviewText: \n",
    "    sent = str(sent, 'utf-8')\n",
    "    sent = re.sub(\"[^\\w]\", \" \",  sent).split()\n",
    "\n",
    "    for word in sent:\n",
    "        words.append(word)\n",
    "\n",
    "# to create a dict of word:frequency\n",
    "counter=collections.Counter(words)\n",
    "\n",
    "# the keys in the dictionary contains unique words.\n",
    "# set of unique words represents the vocabulary\n",
    "vocab = counter.keys()\n",
    "\n",
    "# sort the words based on frequency\n",
    "sortedWords_by_frequency = sorted(\n",
    "        counter.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "# print(sortedWords_by_frequency)\n",
    "\n",
    "# select 'n' words having highest frequency\n",
    "top_words_count = 5000\n",
    "top_words = sortedWords_by_frequency[:top_words_count]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Reviews using Sorted Frequency Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each review based on indices and split into test/train\n",
    "\n",
    "# Doing indexing of top words and storing it in a dictionary\n",
    "top_words_dict = {}\n",
    "index = 1\n",
    "for word_freq in top_words:\n",
    "    top_words_dict[word_freq[0]] =  index\n",
    "    index = index + 1\n",
    "\n",
    "# Convert reviews as list of indices of words\n",
    "indexedReview = []\n",
    "for idx, sent in enumerate(reviewText): \n",
    "    sent = str(sent, 'utf-8')\n",
    "    sent = re.sub(\"[^\\w]\", \" \",  sent).split()\n",
    "        \n",
    "    wordIndices = []\n",
    "    for word in sent:\n",
    "        wordIndex = top_words_dict.get(word, -1)\n",
    "        if (wordIndex > 0):\n",
    "            wordIndices.append(wordIndex)\n",
    "        \n",
    "    indexedReview.append(wordIndices)\n",
    "indexedReview = indexedReview\n",
    "\n",
    "# Split the encoded Amazon reviews to train/test in LSTM\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    indexedReview, y_bin, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1902,
     "status": "ok",
     "timestamp": 1524920680005,
     "user": {
      "displayName": "Applied AI Course",
      "photoUrl": "//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg",
      "userId": "116292885805316472049"
     },
     "user_tz": -330
    },
    "id": "Jr4cugH3mZ_X",
    "outputId": "54015fb5-56bf-4704-e022-f19bc073b500"
   },
   "outputs": [],
   "source": [
    "# Credits: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1524920699702,
     "user": {
      "displayName": "Applied AI Course",
      "photoUrl": "//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg",
      "userId": "116292885805316472049"
     },
     "user_tz": -330
    },
    "id": "eYEE6ts7GAjC",
    "outputId": "e28e2718-8b44-4478-d8b8-88c0189fdde8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 460, 764, 33, 59, 1935, 764, 249, 59, 37, 56, 879, 5, 126, 221, 582, 924, 1935, 764, 4519, 1463, 627, 141, 972, 192, 268, 8, 1935, 764, 71, 910, 1135, 468, 1677, 1243, 78, 18, 929, 217, 1995, 2827, 587, 2873, 787, 1935, 764, 97, 300, 207, 1366, 449, 1258, 2, 764, 59, 2237, 1935, 79, 8, 652, 30, 3913, 652, 764, 273, 188, 147, 728, 816, 2, 111, 8, 300, 25, 15, 9, 652, 510, 184, 764, 30, 1935, 159, 34, 622, 184, 764, 230, 181, 184, 817, 897, 45, 8, 139, 652, 1935, 328, 1068, 1070, 386, 5, 309, 299, 56, 1935, 764, 486, 31, 227, 27, 191, 268, 591, 636, 125, 6, 268, 619, 202, 133, 18, 451, 136, 33, 37, 467, 20, 154, 43, 268, 337, 221, 22, 1213, 1207, 728, 3018, 1935, 764, 696, 268, 550, 1742, 60, 518, 43, 249, 17, 268, 497, 9, 764, 38, 7, 35, 56, 85, 2873, 671, 270, 29, 1454, 85, 37]\n",
      "<class 'list'>\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "# For Sanity Check\n",
    "\n",
    "print(X_train[1])\n",
    "print(type(X_train[1]))\n",
    "print(len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2710,
     "status": "ok",
     "timestamp": 1524920705604,
     "user": {
      "displayName": "Applied AI Course",
      "photoUrl": "//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg",
      "userId": "116292885805316472049"
     },
     "user_tz": -330
    },
    "id": "57N6TyKLH-Pc",
    "outputId": "63967244-074c-46bc-8d83-f084716fbb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 600)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    9  460  764   33   59 1935  764  249   59   37   56  879    5\n",
      "  126  221  582  924 1935  764 4519 1463  627  141  972  192  268    8\n",
      " 1935  764   71  910 1135  468 1677 1243   78   18  929  217 1995 2827\n",
      "  587 2873  787 1935  764   97  300  207 1366  449 1258    2  764   59\n",
      " 2237 1935   79    8  652   30 3913  652  764  273  188  147  728  816\n",
      "    2  111    8  300   25   15    9  652  510  184  764   30 1935  159\n",
      "   34  622  184  764  230  181  184  817  897   45    8  139  652 1935\n",
      "  328 1068 1070  386    5  309  299   56 1935  764  486   31  227   27\n",
      "  191  268  591  636  125    6  268  619  202  133   18  451  136   33\n",
      "   37  467   20  154   43  268  337  221   22 1213 1207  728 3018 1935\n",
      "  764  696  268  550 1742   60  518   43  249   17  268  497    9  764\n",
      "   38    7   35   56   85 2873  671  270   29 1454   85   37]\n"
     ]
    }
   ],
   "source": [
    "# Truncate and/or pad input sequences\n",
    "max_review_length = 600\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "def trainModel(model):\n",
    "    history = model.fit(X_train, np.array(y_train), validation_split=0.33, epochs=10, batch_size=64)\n",
    "\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Single Layer LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1524920710407,
     "user": {
      "displayName": "Applied AI Course",
      "photoUrl": "//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg",
      "userId": "116292885805316472049"
     },
     "user_tz": -330
    },
    "id": "CquzlqrOIYGn",
    "outputId": "3e3104e0-f28f-4ec5-858a-e307c3a8e700"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 600, 32)           160032    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,333\n",
      "Trainable params: 213,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "733/733 [==============================] - 217s 295ms/step - loss: 2.7577 - accuracy: 0.8769 - val_loss: 1.0815 - val_accuracy: 0.8991\n",
      "Epoch 2/10\n",
      "733/733 [==============================] - 214s 292ms/step - loss: 0.5183 - accuracy: 0.9189 - val_loss: 0.2743 - val_accuracy: 0.9124\n",
      "Epoch 3/10\n",
      "733/733 [==============================] - 216s 295ms/step - loss: 0.1947 - accuracy: 0.9304 - val_loss: 0.2226 - val_accuracy: 0.9111\n",
      "Epoch 4/10\n",
      "733/733 [==============================] - 207s 283ms/step - loss: 0.1634 - accuracy: 0.9368 - val_loss: 0.2361 - val_accuracy: 0.9057\n",
      "Epoch 5/10\n",
      "733/733 [==============================] - 207s 282ms/step - loss: 0.1487 - accuracy: 0.9439 - val_loss: 0.2496 - val_accuracy: 0.9075\n",
      "Epoch 6/10\n",
      "733/733 [==============================] - 208s 283ms/step - loss: 0.1361 - accuracy: 0.9493 - val_loss: 0.2560 - val_accuracy: 0.9052\n",
      "Epoch 7/10\n",
      "733/733 [==============================] - 207s 282ms/step - loss: 0.1247 - accuracy: 0.9540 - val_loss: 0.2752 - val_accuracy: 0.9020\n",
      "Epoch 8/10\n",
      "733/733 [==============================] - 208s 283ms/step - loss: 0.1153 - accuracy: 0.9593 - val_loss: 0.2919 - val_accuracy: 0.9006\n",
      "Epoch 9/10\n",
      "733/733 [==============================] - 208s 284ms/step - loss: 0.1052 - accuracy: 0.9635 - val_loss: 0.3048 - val_accuracy: 0.9001\n",
      "Epoch 10/10\n",
      "733/733 [==============================] - 215s 293ms/step - loss: 0.0979 - accuracy: 0.9662 - val_loss: 0.3258 - val_accuracy: 0.9010\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(model1\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#Refer: https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# summarize history for accuracy\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "# regularizers = [L1L2(l1=0.0, l2=0.0), L1L2(l1=0.01, l2=0.0), L1L2(l1=0.0, l2=0.01), L1L2(l1=0.01, l2=0.01)]\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words_count+1, embedding_vector_length, input_length=max_review_length))\n",
    "#model1.add(Dropout(0.75))\n",
    "model1.add(LSTM(100, bias_regularizer=L1L2(l1=0.0, l2=0.05)))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())\n",
    "#Refer: https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model\n",
    "\n",
    "trainModel(model=model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Multiple Layer LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "from keras.layers import Dropout\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words_count, embedding_vector_length, input_length=max_review_length))\n",
    "model2.add(Dropout(0.75))\n",
    "model2.add(LSTM(200, bias_regularizer=L1L2(l1=0.0, l2=0.05), return_sequences=True))\n",
    "model2.add(Dropout(0.75))\n",
    "model2.add(LSTM(150, bias_regularizer=L1L2(l1=0.0, l2=0.05)))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "\n",
    "trainModel(model=model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Multi-Layer Neuron-Dense LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "embedding_vector_length = 32\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(top_words_count, embedding_vector_length, input_length=max_review_length))\n",
    "model2.add(Dropout(0.5))\n",
    "model3.add(LSTM(512, bias_regularizer=L1L2(l1=0.0, l2=0.05), return_sequences=True))\n",
    "model2.add(Dropout(0.5))\n",
    "model3.add(LSTM(256, bias_regularizer=L1L2(l1=0.0, l2=0.05)))\n",
    "model2.add(Dropout(0.4))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model3.summary())\n",
    "\n",
    "trainModel(model=model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Summary Statistics\n",
    "\n",
    " Model  | Architecture | Test Metric |\n",
    "------------- |:----------:|:-----------------:|\n",
    "**M1: Single Layer LSTM** | 100 (1) LSTM stack  | **Accuracy = 89.99%**, Train Accuracy after 10th Epoch= 96.31|\n",
    "**M2: Double Layer LSTM** | 200 (1) -150 (2) LSTM stack | **Accuracy = 91.07%**, Train Accuracy after 10th Epoch= 92.5%|\n",
    "**M3: Multi-Layer Neuron-Dense LSTM** | 512 (1) -256 (2) LSTM stack | **Accuracy = 89.97%**, Train Accuracy after 10th Epoch= 93.7%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Three architectures with single layer and double layer LSTMs are used to train frequency-encoded Amazon Review dataset.\n",
    "2. **Double-Layer LSTM Architecture obtained highest accuracy** on validation dataset.\n",
    "3. A **single layer stack of 100 LSTMs (M1) fetched a commendable validation accuracy** of 89.99%.\n",
    "4. The **validation accuracy of multi layer neuron dense LSTM stack (M3) fell to 89.97%**, though it showed a hike in training accuracy, 93.7%. The increase in training accuracy and reduction in test accuracy points to slight overfitting on the train data.\n",
    "5. The **slight improvement in accuracy of Model 2 may not be worth the extra time spent on training such a stack-dense model. Hence, Model 1 with an accuracy of 90% is the architecture of choice.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "LSTM_IMDB.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
